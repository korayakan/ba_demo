{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anhang.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkATudyL8XRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get -qq install tesseract-ocr\n",
        "!pip install -q pytesseract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UDxP0JS84Oh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q fuzzywuzzy[speedup]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fMikmAR88nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "![ -d \"ba_dataset\" ] && echo \"Dataset directory exists.\"\n",
        "![ ! -d \"ba_dataset\" ] && echo \"Dataset directory DOES NOT exist. Cloning from Github...\" && git clone -q https://github.com/korayakan/ba_dataset.git && rm -rf ba_dataset/.git && echo \"Done\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEw650Pk9inr",
        "colab_type": "text"
      },
      "source": [
        "DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiGTSRsZ9DMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for image handling\n",
        "from PIL import Image\n",
        "# for json\n",
        "import json\n",
        "# for glob file search\n",
        "import glob\n",
        "# for fuzzy string comparison\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "# for encoding strings\n",
        "from zlib import crc32\n",
        "# for log configuration\n",
        "import logging\n",
        "# for random numbers\n",
        "from random import randint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCCwkFDL9z5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "COORDINATE_PATH = 'ba_dataset/SROIE2019/0325updated.task1train(626p)'\n",
        "TAG_PATH = 'ba_dataset/SROIE2019/0325updated.task2train(626p)'\n",
        "IMG_PATH = COORDINATE_PATH\n",
        "TAG_TO_IDX = {'': 0, 'company': 1, 'date': 2, 'address': 3, 'total': 4}\n",
        "IDX_TO_TAG = {v: k for k, v in TAG_TO_IDX.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whGh25vc9_df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_filenames(path, suffix):\n",
        "    path = path + '/' if not path.endswith('/') else path\n",
        "    files = glob.glob(path + '*.' + suffix)\n",
        "    for idx, file in enumerate(files):\n",
        "        files[idx] = file.split(\"/\")[-1].replace('.txt', '').replace('.jpg', '')\n",
        "    return files\n",
        "\n",
        "\n",
        "def get_text_filenames(path):\n",
        "    return get_filenames(path, 'txt')\n",
        "\n",
        "\n",
        "def get_image_filenames(path):\n",
        "    return get_filenames(path, 'jpg')\n",
        "\n",
        "\n",
        "def prepare_data(filename):\n",
        "    tags = read_tags(filename)\n",
        "\n",
        "    im = read_image_file(filename)\n",
        "    width, height = im.size\n",
        "\n",
        "    coordinates = read_normalized_coordinates(filename, width, height)\n",
        "    coordinate_inputs = [coordinates[i][:9] for i in range(len(coordinates))]\n",
        "    coordinate_texts = [coordinates[i][9] for i in range(len(coordinates))]\n",
        "\n",
        "    coordinate_tags = match_coordinate_tags(coordinate_texts, tags)\n",
        "    return tags, coordinate_inputs, coordinate_texts, coordinate_tags\n",
        "\n",
        "\n",
        "def get_all_filenames():\n",
        "    image_files = get_image_filenames(IMG_PATH)\n",
        "    # print('found {} image files'.format(len(image_files)))\n",
        "\n",
        "    coordinate_files = get_text_filenames(COORDINATE_PATH)\n",
        "    # print('found {} files with coordinate data'.format(len(coordinate_files)))\n",
        "\n",
        "    tag_files = get_text_filenames(TAG_PATH)\n",
        "    # print('found {} files with tag data'.format(len(tag_files)))\n",
        "\n",
        "    filenames = list(set(image_files) & set(coordinate_files) & set(tag_files))\n",
        "    filenames.sort()\n",
        "    return filenames\n",
        "\n",
        "\n",
        "def prepare_training_data():\n",
        "    filenames = get_all_filenames()\n",
        "    print('found {} files with coordinate and tag data'.format(len(filenames)))\n",
        "\n",
        "    training_size = int(len(filenames) * 0.8)\n",
        "    print('using {} files for training'.format(training_size))\n",
        "\n",
        "    training_data = []\n",
        "    for i in range(training_size):\n",
        "        tags, coordinate_inputs, coordinate_texts, coordinate_tags = prepare_data(filenames[i])\n",
        "        training_data.append((coordinate_inputs, coordinate_tags))\n",
        "\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def get_random_test_file():\n",
        "    filenames = get_all_filenames()\n",
        "    print('found {} files with coordinate and tag data'.format(len(filenames)))\n",
        "\n",
        "    test_size = int(len(filenames) * 0.2)\n",
        "    print('using {} files for testing'.format(test_size))\n",
        "    \n",
        "    return filenames[randint(test_size + 1, len(filenames) - 1)]\n",
        "\n",
        "\n",
        "def read_text_file(path, filename):\n",
        "    path = path + '/' if not path.endswith('/') else path\n",
        "    with open(path + filename + '.txt') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "\n",
        "def read_text_file_lines(path, filename):\n",
        "    path = path + '/' if not path.endswith('/') else path\n",
        "    lines = []\n",
        "    with open(path + filename + '.txt') as file:\n",
        "        for line in file:\n",
        "            lines.append(line.rstrip('\\n'))\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_coordinates(filename, path='ba_dataset/SROIE2019/0325updated.task1train(626p)'):\n",
        "    text = read_text_file_lines(path, filename)\n",
        "    coordinates = []\n",
        "    for line in text:\n",
        "        tokens = line.split(',')\n",
        "        line_coordinates = list(map(int, tokens[0:8]))\n",
        "        line_text = ','.join(tokens[8:])\n",
        "        line_coordinates.append(line_text)\n",
        "        coordinates.append(line_coordinates)\n",
        "    return coordinates\n",
        "\n",
        "\n",
        "def read_normalized_coordinates(filename, width, height, path='ba_dataset/SROIE2019/0325updated.task1train(626p)'):\n",
        "    coordinates = read_coordinates(filename, path=path)\n",
        "    for line in coordinates:\n",
        "        for x in range(0, 8, 2):\n",
        "            line[x] /= width\n",
        "        for x in range(1, 8, 2):\n",
        "            line[x] /= height\n",
        "        line.append(line[8])\n",
        "        line[8] = normalize_text(line[8])\n",
        "    return coordinates\n",
        "\n",
        "\n",
        "def read_tags(filename, path='ba_dataset/SROIE2019/0325updated.task2train(626p)'):\n",
        "    return json.loads(read_text_file(path, filename))\n",
        "\n",
        "\n",
        "def read_image_file(filename, path='ba_dataset/SROIE2019/0325updated.task1train(626p)'):\n",
        "    path = path + '/' if not path.endswith('/') else path\n",
        "    # return cv2.imread(path + filename + '.jpg', 0)\n",
        "    return Image.open(path + filename + '.jpg')\n",
        "\n",
        "\n",
        "def match_coordinate_tags(coordinate_texts, tags):\n",
        "\n",
        "    tags_reverted = {v: k for k, v in tags.items()}\n",
        "    tag_values = list(tags.values())\n",
        "    coordinate_tags = []\n",
        "    for text in coordinate_texts:\n",
        "        # text = text.replace('*', '_').replace('%', '_').replace(':', '_').replace('=', '_').\n",
        "        # replace('-', '_').replace('(', '_').replace('@', '_').replace('^', '_').replace('/', '_')\n",
        "        tag_guess = process.extractOne(text, tag_values, scorer=fuzz.partial_ratio, score_cutoff=90)\n",
        "        tag = ''\n",
        "        if tag_guess is not None:\n",
        "            tag = tags_reverted[tag_guess[0]]\n",
        "        coordinate_tags.append(encode_tag(tag))\n",
        "    return coordinate_tags\n",
        "\n",
        "\n",
        "def encode_tag(tag):\n",
        "    return TAG_TO_IDX[tag]\n",
        "\n",
        "\n",
        "def decode_tag(tag_idx):\n",
        "    return IDX_TO_TAG[tag_idx]\n",
        "\n",
        "\n",
        "def normalize_text(input_text, encoding=\"utf-8\"):\n",
        "    # see https://stackoverflow.com/questions/40351791/how-to-hash-strings-into-a-float-in-01\n",
        "    return float(crc32(input_text.encode(encoding)) & 0xffffffff) / 2**32\n",
        "\n",
        "\n",
        "def combine_predicted_tags(texts, tags):\n",
        "    company = ''\n",
        "    date = ''\n",
        "    address = ''\n",
        "    total = ''\n",
        "\n",
        "    for i in range(len(tags)):\n",
        "        if tags[i] == 1 and not company.endswith(texts[i]):\n",
        "            company += ' ' + texts[i]\n",
        "        if tags[i] == 2 and not date.endswith(texts[i]):\n",
        "            date += ' ' + texts[i]\n",
        "        if tags[i] == 3 and not address.endswith(texts[i]):\n",
        "            address += ' ' + texts[i]\n",
        "        if tags[i] == 4 and not total.endswith(texts[i]):\n",
        "            total += ' ' + texts[i]\n",
        "\n",
        "    return {'company': company.strip(), 'date': date.strip(), 'address': address.strip(), 'total': total.strip()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVHqX5Ob-QHS",
        "colab_type": "text"
      },
      "source": [
        "LSTM Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNxb2_d6-Gcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as opt\n",
        "import torch.nn.functional as fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzE1aLZ--ayK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SERIALIZED_MODEL_NAME = 'ba_model.pt'\n",
        "\n",
        "INPUT_SIZE = 9\n",
        "OUTPUT_SIZE = 5\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size=6, num_of_layers=1):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # The LSTM takes coordinates as input, and outputs hidden states\n",
        "        self.lstm = nn.LSTM(INPUT_SIZE, hidden_size, num_layers=num_of_layers)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_size, OUTPUT_SIZE)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        hidden_space, _ = self.lstm(input_seq)\n",
        "        tag_space = self.hidden2tag(hidden_space.view(len(input_seq), -1))\n",
        "        tag_scores = fn.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "def load_model(model_path=SERIALIZED_MODEL_NAME):\n",
        "    if os.path.isfile(model_path):\n",
        "        model = torch.load(model_path)\n",
        "        model.eval()\n",
        "        return model\n",
        "    else:\n",
        "        print('Model was not trained yet!')\n",
        "        sys.exit(0)\n",
        "\n",
        "\n",
        "def evaluate(coordinate_inputs, model_path=SERIALIZED_MODEL_NAME):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # print('Using {} for prediction'.format(device))\n",
        "    with torch.no_grad():\n",
        "        model = load_model(model_path)\n",
        "        model.to(device)\n",
        "        input_seq = torch.tensor(coordinate_inputs)\n",
        "        input_seq = input_seq.unsqueeze(1)\n",
        "        input_seq = input_seq.to(device)\n",
        "        tag_scores = model(input_seq)\n",
        "        probabilities, tags = tag_scores.topk(1)\n",
        "        predictions = []\n",
        "        for i in range(len(probabilities)):\n",
        "            predictions.append((torch.exp(probabilities[i]).item(), tags[i].item()))\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYALmQrV-rdx",
        "colab_type": "text"
      },
      "source": [
        "TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knf3Okr_-gHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, print_every=1, learning_rate=0.1, hidden_size=6, num_of_layers=1):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print('Using {} for training'.format(device))\n",
        "\n",
        "    model = LSTM(hidden_size=hidden_size, num_of_layers=num_of_layers)\n",
        "    loss_function = nn.NLLLoss()\n",
        "    optimizer = opt.SGD(model.parameters(), lr=learning_rate)\n",
        "    model.to(device)\n",
        "\n",
        "    data_set = prepare_training_data()\n",
        "    split_data = {0: [], 1: [], 2: [], 3: []}\n",
        "    for i in range(len(data_set)):\n",
        "        if i % 4 == 0:\n",
        "            split_data[0].append(data_set[i])\n",
        "        if i % 4 == 1:\n",
        "            split_data[1].append(data_set[i])\n",
        "        if i % 4 == 2:\n",
        "            split_data[2].append(data_set[i])\n",
        "        if i % 4 == 3:\n",
        "            split_data[3].append(data_set[i])\n",
        "    # for i in range(4):\n",
        "    #     print(len(split_data[i]))\n",
        "\n",
        "    steps = 0\n",
        "    running_loss = 0\n",
        "    train_losses, test_losses = [], []\n",
        "    for epoch in range(epochs):\n",
        "        steps += 1\n",
        "        validation_idx = epoch % 4\n",
        "        validation_data = split_data[validation_idx]\n",
        "        training_data = []\n",
        "        for i in range(4):\n",
        "            if i != validation_idx:\n",
        "                training_data.extend(split_data[i])\n",
        "\n",
        "        for coordinate_inputs, coordinate_tags in training_data:\n",
        "            # print(coordinate_inputs)\n",
        "            # print(coordinate_tags)\n",
        "\n",
        "            # Step 1. Clear Pytorch gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Step 2. Get inputs ready for the network\n",
        "            input_seq = torch.tensor(coordinate_inputs)\n",
        "            input_seq = input_seq.unsqueeze(1)\n",
        "            input_seq = input_seq.to(device)\n",
        "            targets = torch.tensor(coordinate_tags)\n",
        "            targets = targets.to(device)\n",
        "            # print(input_seq)\n",
        "            # print(targets)\n",
        "\n",
        "            # Step 3. Run forward pass\n",
        "            tag_scores = model(input_seq)\n",
        "\n",
        "            # Step 4. Compute the loss, gradients, and update the parameters\n",
        "            loss = loss_function(tag_scores, targets)\n",
        "            # print(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if (steps == 1) or (steps % print_every == 0):\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for coordinate_inputs_val, coordinate_tags_val in validation_data:\n",
        "                    input_seq_val = torch.tensor(coordinate_inputs_val)\n",
        "                    input_seq_val = input_seq_val.unsqueeze(1)\n",
        "                    input_seq_val = input_seq_val.to(device)\n",
        "                    targets_val = torch.tensor(coordinate_tags_val)\n",
        "                    targets_val = targets_val.to(device)\n",
        "\n",
        "                    tag_scores_val = model.forward(input_seq_val)\n",
        "                    loss_val = loss_function(tag_scores_val, targets_val)\n",
        "                    test_loss += loss_val.item()\n",
        "\n",
        "            train_losses.append(running_loss / (len(training_data) * print_every))\n",
        "            test_losses.append(test_loss / len(validation_data))\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}.. \"\n",
        "                  f\"Train loss: {running_loss / (len(training_data) * print_every):.3f}.. \"\n",
        "                  f\"Test loss: {test_loss / len(validation_data):.3f}.. \")\n",
        "                  #f\"Test accuracy: {accuracy / len(testloader):.3f}\")\n",
        "            running_loss = 0\n",
        "            model.train()\n",
        "\n",
        "    torch.save(model, SERIALIZED_MODEL_NAME)\n",
        "    return train_losses, test_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7o3MzEp--TU",
        "colab_type": "text"
      },
      "source": [
        "PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tljDdYzD_AH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(filename, model_path=SERIALIZED_MODEL_NAME):\n",
        "    print('File:')\n",
        "    print(filename, '\\n')\n",
        "\n",
        "    tags, coordinate_inputs, coordinate_texts, coordinate_tags = prepare_data(filename)\n",
        "\n",
        "    print('Expected category tags:')\n",
        "    print(tags, '\\n')\n",
        "\n",
        "    # print('Input coordinates:')\n",
        "    # print(coordinate_inputs)\n",
        "    print(coordinate_inputs)\n",
        "    print(coordinate_texts)\n",
        "    print('Expected coordinate tags:')\n",
        "    print(coordinate_tags)\n",
        "\n",
        "    predictions = evaluate(coordinate_inputs, model_path)\n",
        "    predicted_coordinate_tags = []\n",
        "    for prediction in predictions:\n",
        "        predicted_coordinate_tags.append(prediction[1])\n",
        "    probabilities = []\n",
        "    for prediction in predictions:\n",
        "        probabilities.append(\"{:.0%}\".format(prediction[0]))\n",
        "    print('Predicted coordinate tags:')\n",
        "    print(predicted_coordinate_tags)\n",
        "    print('Confidence:')\n",
        "    print(probabilities, '\\n')\n",
        "\n",
        "    predicted_tags = combine_predicted_tags(coordinate_texts, predicted_coordinate_tags)\n",
        "    print(predicted_tags)\n",
        "\n",
        "\n",
        "def get_expected_tags(filename):\n",
        "    tags, coordinate_inputs, coordinate_texts, coordinate_tags = prepare_data(filename)\n",
        "    return coordinate_tags\n",
        "\n",
        "\n",
        "def get_predicted_tags(filename, model_path=SERIALIZED_MODEL_NAME):\n",
        "    tags, coordinate_inputs, coordinate_texts, coordinate_tags = prepare_data(filename)\n",
        "    predictions = evaluate(coordinate_inputs, model_path)\n",
        "    predicted_coordinate_tags = []\n",
        "    for prediction in predictions:\n",
        "        predicted_coordinate_tags.append(prediction[1])\n",
        "    return predicted_coordinate_tags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw8GLZWI_XLN",
        "colab_type": "text"
      },
      "source": [
        "OCR DEMO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z7oJi9J_ZmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = read_image_file('X00016469612')\n",
        "img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh5t6Gn5_mhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytesseract\n",
        "print(pytesseract.image_to_string(img, lang='eng', config='--oem 1'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh0JJozi_9G0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(pytesseract.image_to_data(img, lang='eng', config='--oem 1'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb1PEwceACx3",
        "colab_type": "text"
      },
      "source": [
        "KI_TRAIN DEMO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOj1-ZW2APga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime, timezone\n",
        "import time\n",
        "start = time.time()\n",
        "now = datetime.now(timezone.utc).astimezone()\n",
        "print('start model training at {}'.format(now))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xwP78t5AdSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_losses, test_losses = train(1000, print_every = 100, learning_rate = 0.05, hidden_size=256, num_of_layers=3)\n",
        "train_losses, test_losses = train(10, print_every = 1, learning_rate = 0.05, hidden_size=256, num_of_layers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCCGAIKvLM1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "now = datetime.now(timezone.utc).astimezone()\n",
        "done = time.time()\n",
        "elapsed = int(round(done - start))\n",
        "print('training finished at {}, duration was {} min {} sec'.format(now, int(round(elapsed / 60)), elapsed % 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mPi-vvKLSam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(test_losses, label='Validation loss')\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4vE1slMLViz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(get_random_test_file())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}